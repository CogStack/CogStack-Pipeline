## ACTIVE SPRING PROFILES
##
spring.profiles.active = jdbc_in,elasticsearchRest,localPartitioning



#### SOURCE: DB CONFIGURATIONS
##
source.JdbcPath = jdbc:postgresql://samples-db:5432/db_samples
source.Driver = org.postgresql.Driver
source.username = test
source.password = test

# optional (default: 10): number of allocated connections to source DB (kept until the end of processing)
#source.poolSize = 10

# The principle SQL block that specifies data to process. Composed of three parts.
source.selectClause = SELECT *
source.fromClause = FROM observations_view
source.sortKey = observation_id

# The principle DB column label mapping for Document data model
source.primaryKeyFieldValue = observation_id
source.timeStamp = observation_timestamp

# Since different DBMS products interpret the SQL standard for time differently, is is necessary to explicitly specify
# the date type that the database is using. E.G. postgres=TIMESTAMP, SQL SERVER=DATETIME
source.dbmsToJavaSqlTimestampType = TIMESTAMP


##### SINK: ELASTICSEARCH CONFIGURATION
##
elasticsearch.cluster.host = elasticsearch-1
elasticsearch.cluster.port = 9200

# optional: store data into this index
elasticsearch.index.name = sample_observations_view

# optional: if the input SQL query returns columns with these labels, ignore them
elasticsearch.excludeFromIndexing = observation_id



#### JOB REPO DB CONFIGURATIONS
##
jobRepository.JdbcPath = jdbc:postgresql://cogstack-job-repo:5432/cogstack
jobRepository.Driver = org.postgresql.Driver
jobRepository.username = cogstack
jobRepository.password = mysecretpassword


#### JOB AND STEP CONFIGURATION
##
# optional (default: 50): commit interval in step - process this many rows before committing results. default 50
#step.chunkSize = 50
# optional (default: 5): number of exceptions that can be thrown before job fails. default 5
#step.skipLimit = 5

# optional (default: 2): Asynchonous TaskExecutor Thread pool size - for multithreading partitions
#step.concurrencyLimit = 8



#### PARTITIONER CONFIGURATION
##
## This is used to inform how the total row count per job should be broken into
## seperate partitions
##
# Two partitioner types are available, either using primary keys (PK) or timestamps and primary keys (PKTimeStamp)
# If using the scheduler, the PKTimeStamp type should be configured
partitioner.partitionType = PKTimeStamp

# optional (default: 1): number of partitions to generate (x / total job row count)
#partitioner.gridSize = 1

# name of timestamp column used for partitioning and checking for new data (only if scheduling is used)
partitioner.timeStampColumnName = observation_timestamp

# name of PK column used for partitioning and checking for new data
# only use with scheduling if PKs are guaranteed to be generated sequentially
partitioner.pkColumnName = observation_id

# this is the table containing the primary keys and optionally, timestamps
partitioner.tableToPartition = observations_view



## SCHEDULER CONFIGURATION
##
# optional (default: false): if true, run a new job after the last one has finished - new jobs will continute to be created indefinitely
#scheduler.useScheduling = false
