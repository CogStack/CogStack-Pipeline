## Spring profiles selection -- used components
##
spring.profiles.active=jdbc_in,jdbc_out,postgres,localPartitioning,tika



#### SOURCE TARGET DB CONFIGURATIONS
##
source.JdbcPath      = jdbc:postgresql://pgsamples:5432/db_samples
source.Driver        = org.postgresql.Driver
source.username      = test
source.password      = test
source.idleTimeout   = 30000
source.maxLifetime   = 60000
source.leakDetectionThreshold = 0


## DB column label mapping for table metadata. Required.
##
# these fields are used for meta-data generation
source.srcTableName = cog_src_table_name
source.srcColumnFieldName = cog_src_field_name
source.primaryKeyFieldName = cog_pk_field_name
source.primaryKeyFieldValue = cog_pk
source.timeStamp = cog_update_time

# number of allocated connections to source DB (kept until the end of processing)
source.poolSize = 10

#Since different DBMS products interpret the SQL standard for time differently, is is necessary to explicitly specify
#the date type that the database is using. E.G. postgres=TIMESTAMP, SQL SERVER=DATETIME
source.dbmsToJavaSqlTimestampType = TIMESTAMP

#The principle SQL block that specifies data to process. Composed of three parts.
source.selectClause = SELECT *
source.sortKey = cog_pk
source.fromClause = FROM reports_binary_view

#paging item reader configuration -- number of rows per page
source.pageSize = 2



#### POSTGRES TARGET DB CONFIGURATIONS
##
target.JdbcPath      = jdbc:postgresql://pgsamples:5432/db_samples
target.Driver        = org.postgresql.Driver
target.username      = test
target.password      = test
target.idleTimeout   = 30000
target.maxLifetime   = 60000

# used Document model for data inserting
target.Sql = INSERT INTO medical_reports_processed (cid, dct, output) VALUES ( CAST( :primaryKeyFieldValue AS integer ), :timeStamp, :outputData)



## TIKA CONFIGURATION
##
## TikaItemProcesser - XHTML or plaintext
tika.keepTags = false
# field with binary content for tika conversion, or field containing path name
tika.binaryFieldName = cog_binary_doc
# fieldname for tika output
tika.tikaFieldName = tika_output
# either database or fileSystemWithDBPath for Docman type systems
tika.binaryContentSource = database
#drive prefix (tests use file on classpath)


## Job Repo DB CONFIGURATIONS
##
##
jobRepository.JdbcPath      = jdbc:postgresql://postgres:5432/cogstack
jobRepository.Driver        = org.postgresql.Driver
jobRepository.username      = cogstack
jobRepository.password      = mysecretpassword
jobRepository.idleTimeout   = 30000
jobRepository.maxLifetime   = 60000


#### JOB AND STEP CONFIGURATION
##
#commit interval in step - process this many rows before committing results. default 50
step.chunkSize = 50
#number of exceptions that can be thrown before job fails. default 5
step.skipLimit = 5

#Asynchonous TaskExecutor Thread pool size - for multithreading partitions
step.concurrencyLimit = 2

#job should complete before this many ms or it will be listed as a failure.
job.jobTimeout = 10000000

#globally unique job name. default is defaultJob
job.jobName = reports_binary_view

#since some JDBC drivers don't support socket timeouts, they can't detect network failures.
#This workaround allows us to specify a global socket timeout for the JVM without diving into the OS socket configuration.
#However, the downside is that all sockets are affected in the JVM, so may not always be appropriate to set this. Comment out to
#not use this global setting
job.globalSocketTimeout = 30000


#### Partitioner Configuration
## This is used to inform how the total row count per job should be broken into
## seperate partitions
##
#Two partitioner types are available, either using primary keys (PK) or timestamps and primary keys (PKTimeStamp)
#If using the scheduler, the PKTimeStamp type should be configured
partitioner.partitionType = PKTimeStamp

#number of partitions to generate (x / total job row count)
partitioner.gridSize = 1

#ms for partition handler to complete partitioning before an exception is thrown
partitioner.partitionHandlerTimeout = 10000000

#name of timestamp column used for partitioning and checking for new data (only if scheduling is used)
partitioner.timeStampColumnName = cog_update_time

#name of PK column used for partitioning and checking for new data
#only use with scheduling if PKs are guaranteed to be generated sequentially
partitioner.pkColumnName = cog_pk

#this is the table containing the primary keys and optionally, timestamps
partitioner.tableToPartition = reports_binary_view



## Scheduler CONFIGURATION
##
#if true, run a new job after the last one has finished - new jobs will continute to be created indefinitely
scheduler.useScheduling = false
