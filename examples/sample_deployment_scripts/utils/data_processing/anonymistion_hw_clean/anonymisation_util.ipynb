{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[<ipython-input-2-a2a9d3c5f3b3>:127] 2021-02-02 19:13:31,619 anonymised EDT_1344208.txt saved to ./anonymised/\n",
      "[<ipython-input-2-a2a9d3c5f3b3>:127] 2021-02-02 19:13:31,642 anonymised EDT_884307.txt saved to ./anonymised/\n",
      "[<ipython-input-2-a2a9d3c5f3b3>:127] 2021-02-02 19:13:31,660 anonymised EDT_1327876.txt saved to ./anonymised/\n",
      "[<ipython-input-2-a2a9d3c5f3b3>:133] 2021-02-02 19:13:31,680 sensitive data saved to ./anonymised/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"rule based anonymisation\"\"\"\n",
    "\n",
    "# __author__      = \"Silverash Wu\"\n",
    "# __copyright__   = \"Copyright 2020, Planet Earth\"\n",
    "\n",
    "import utils\n",
    "import json\n",
    "import re\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "class AnonymiseRule(object):\n",
    "    def __init__(self, rule_file):\n",
    "        self._rules = utils.load_json_data(rule_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def rul_extraction(full_text, re_objs):\n",
    "        results = []\n",
    "        for ro in re_objs:\n",
    "            if 'disabled' in ro and ro['disabled']:\n",
    "                continue\n",
    "            flag = 0\n",
    "            if 'multiline' in ro['flags']:\n",
    "                flag |= re.MULTILINE\n",
    "            if 'ignorecase' in ro['flags']:\n",
    "                flag |= re.IGNORECASE\n",
    "            matches = re.finditer(ro['pattern'], full_text, flag)\n",
    "            for m in matches:\n",
    "                ret = {'type': ro['data_type'], 'attrs': {}}\n",
    "                results.append(ret)\n",
    "                ret['attrs']['full_match'] = m.group(0)\n",
    "                ret['pos'] = m.span()\n",
    "                i = 1\n",
    "                if 'data_labels' in ro:\n",
    "                    for attr in ro['data_labels']:\n",
    "                        ret['attrs'][attr] = m.group(i)\n",
    "                        i += 1\n",
    "        return results\n",
    "\n",
    "    def do_letter_parsing(self, full_text):\n",
    "        re_exps = self._rules\n",
    "        results = []\n",
    "        header_pos = -1\n",
    "        tail_pos = -1\n",
    "        header_result = self.rul_extraction(full_text, [re_exps['letter_header_splitter']])\n",
    "        tail_result = self.rul_extraction(full_text, [re_exps['letter_end_splitter']])\n",
    "        results += header_result\n",
    "        if len(header_result) > 0:\n",
    "            header_pos = header_result[0]['pos'][0]\n",
    "            header_text = full_text[:header_pos]\n",
    "            phone_results = self.rul_extraction(header_text, re_exps['phone'])\n",
    "            dr_results = self.rul_extraction(header_text, [re_exps['doctor']])\n",
    "            results += phone_results\n",
    "            results += dr_results\n",
    "        if len(tail_result) > 0:\n",
    "            tail_pos = tail_result[0]['pos'][1]\n",
    "            tail_text = full_text[tail_pos:]\n",
    "            for sent_type in re_exps['sent_rules']:\n",
    "                results += self.rul_extraction(tail_text, re_exps[sent_type])\n",
    "        return results, header_pos, tail_pos\n",
    "\n",
    "    def do_full_text_parsing(self, full_text):\n",
    "        re_exps = self._rules\n",
    "        matched_rets = []\n",
    "        for st in re_exps['sent_rules']:\n",
    "            rules = re_exps['sent_rules'][st]\n",
    "            matched_rets += self.rul_extraction(full_text, rules if type(rules) is list else [rules])\n",
    "        return matched_rets, 0, 0\n",
    "\n",
    "    @staticmethod\n",
    "    def do_replace(text, pos, sent_text, replace_char='x'):\n",
    "        return text[:pos] + re.sub(r'[^\\n\\s]', 'x', sent_text) + text[pos+len(sent_text):]\n",
    "\n",
    "\n",
    "def anonymise_doc(doc_id, text, failed_docs, anonymis_inst, sent_container):\n",
    "    \"\"\"\n",
    "    anonymise a document\n",
    "    :param doc_id:\n",
    "    :param text:\n",
    "    :param failed_docs:\n",
    "    :param anonymis_inst: anonymise_rule instance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # rets = do_letter_parsing(text)\n",
    "    rets = anonymis_inst.do_full_text_parsing(text)\n",
    "    if rets[1] < 0 or rets[2] < 0:\n",
    "        failed_docs.append(doc_id)\n",
    "        logging.info('````````````` %s failed' % doc_id)\n",
    "        return None, None\n",
    "    else:\n",
    "        sen_data = rets[0]\n",
    "        anonymised_text = text\n",
    "        for d in sen_data:\n",
    "            if 'name' in d['attrs']:\n",
    "                logging.debug('removing %s [%s] ' % (d['attrs']['name'], d['type']))\n",
    "                if is_valid_place_holder(d['attrs']['name']):\n",
    "                    anonymised_text = AnonymiseRule.do_replace(anonymised_text, d['pos'][0] + d['attrs']['full_match'].find(d['attrs']['name']), d['attrs']['name'])\n",
    "                    # 'x' * len(d['attrs']['name']))\n",
    "                sent_container.append({'type': d['type'], 'sent': d['attrs']['name']})\n",
    "            if 'number' in d['attrs']:\n",
    "                logging.debug ('removing %s ' % d['attrs']['number'])\n",
    "                if is_valid_place_holder(d['attrs']['number']):\n",
    "                    anonymised_text = AnonymiseRule.do_replace(anonymised_text, d['pos'][0], d['attrs']['number'])\n",
    "                sent_container.append({'type': d['type'], 'sent': d['attrs']['number']})\n",
    "        return anonymised_text, sen_data\n",
    "\n",
    "\n",
    "def is_valid_place_holder(s):\n",
    "    return len(s) >= 2\n",
    "\n",
    "\n",
    "def dir_anonymisation(folder, rule_file, output_folder=None):\n",
    "    anonymis_inst = AnonymiseRule(rule_file)\n",
    "    onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f)) and f.endswith('.txt')]\n",
    "    container = []\n",
    "    sent_data = []\n",
    "    for f in onlyfiles:\n",
    "        text = utils.read_text_file_as_string(join(folder, f), encoding='cp1252')\n",
    "        anonymised, sensitive_data = anonymise_doc(f, text, container, anonymis_inst, sent_data)\n",
    "        if output_folder is not None:\n",
    "            utils.save_string(anonymised, join(output_folder, f))\n",
    "            logging.info('anonymised %s saved to %s' % (f, output_folder))\n",
    "        else:\n",
    "            logging.info('[anonymised %s]:\\n%s\\n\\n' % (f, anonymised))\n",
    "        sent_data.append(sensitive_data)\n",
    "    if output_folder is not None:\n",
    "        utils.save_json_array(sent_data, join(output_folder, 'sensitive_data.json'))\n",
    "        logging.info('sensitive data saved to %s' % output_folder)\n",
    "    else:\n",
    "        logging.info('sensitive data:\\n%s' % json.dumps(sent_data))\n",
    "    return sent_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level='INFO', format='[%(filename)s:%(lineno)d] %(asctime)s %(message)s')\n",
    "    dir_anonymisation('./files/',\n",
    "                      './conf/anonymise_rules.json',\n",
    "                      './anonymised/'\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
