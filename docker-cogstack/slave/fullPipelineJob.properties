# EXAMPLE CONFIG FOR deidentification Job 
#####Elasticsearch CONFIGURATION####
elasticsearch.cluster.name = elasticsearch
elasticsearch.cluster.host =  172.17.0.1
elasticsearch.cluster.port = 9300
elasticsearch.shield.enabled = false
elasticsearch.shield.user = <user>:<password>
elasticsearch.shield.ssl.keystore.path = /home/rich/elk/ssh-keystores/tempnode/tempnode.jks
elasticsearch.shield.ssl.keystore.password = <password>
elasticsearch.shield.ssl.truststore.path = /home/rich/elk/ssh-keystores/tempnode/tempnode.jks
elasticsearch.shield.ssl.truststore.password = <password>
elasticsearch.shield.transport.ssl = true
elasticsearch.index.name = test_index2
elasticsearch.type = test_type
elasticsearch.response.timeout = 1000000
#do not index these column labels if detected
elasticsearch.excludeFromIndexing = binaryContent,primaryKeyFieldName
#specify the JODA date pattern that is compatible with the elasticsearch dynamic file mapping for dates
datePatternForES = yyyy-MM-dd'T'HH:mm:ss.SSS
#####DEIDENTIFICATION CONFIGURATION####
#mask text in documents, such as patient identifiers, using a GATE app or the KCL ElasticGazetteer

#GATE app location of DE-ID
deIdApp = /home/rich/gate_apps/ElasticGazetteer/application.xgapp

#comma separated field names to process. Maps to database column labels
fieldsToDeId = someText

#if true, replace the text strings in the fields listed above. If false, create a new field called deidentified_<originalFieldName>
replaceFields = false

#ElasticGazetteer config for text de-identification if not using a GATE app
useGateApp = false


#ElasticGazetteer only

#levenstein distance between 1 and 100. Higher = greater fuzziness
levDistance = 30

#don't mask words shorter than this
minWordLength = 3

#SQL query to retrieve strings of text for masking. E.g. names, addresses etc. Must surround document primary key
# as mapped elsewhere
stringTermsSQLFront = select identifier from vwIdentifiers where primaryKeyFieldValue =

stringTermsSQLBack =

#As above, but with date/time values
timestampTermsSQLFront = select DATE_OF_BIRTH from tblIdentifiers where primaryKeyFieldValue =

timestampTermsSQLBack =

fileOutputDirectory = /home/rich/junk/
#####NIOLARK CONFIGURATION####
biolarkEndPoint = http://lb:80/cr/annotate

#comma seperated
fieldsToBioLark = someText

#name of new field created by biolark
biolarkFieldName = biolark

#ms to connect to a biolark WS
biolarkConnectTimeout = 2000

#ms for biolark to respond
biolarkReadTimeout = 30000

#if biolark fails, retry for this long before giving up
biolarkRetryTimeout = 5000

#wait this long before retrys
biolarkRetryBackoff = 3000
#####JMS CONFIGURATION####
#for remote partitioning
#IP of JMS server
jmsIP = tcp://activemq:61616?jms.prefetchPolicy.all=1
jmsUsername = admin
jmsPassword = your_password
##timeout to prevent a hung client
closeTimeout = 100000
requestQueueName = basicReqChannel
replyQueueName = basicReplyQueue
#####JOB AND STEP CONFIGURATION####
# commit interval in step
chunkSize = 50
#number of exceptions that can be thrown before job fails.
skipLimit = 5

#Asynchonous TaskExecutor Thread pool size - for multithreading partitions
concurrencyLimit = 16

# DB column label mapping for metadata
srcTableName = srctablename
srcColumnFieldName = srcColumnFieldName
primaryKeyFieldName = primaryKeyFieldName
primaryKeyFieldValue = primaryKeyFieldValue
timeStamp = updateTime

#job should complete before this many ms or it will fail
jobTimeout = 10000000

#globally unique job name
jobName = aDockerJob4

#since some JDBC drivers don't support socket timeouts, they can't detect network failures.
#This workaround allows us to specify a global socket timeout for the JVM without diving into the OS socket configuration.
#However, the downside is that all sockets are affected in the JVM, so may not always be appropriate to set this. Comment out to
#not use this global setting
globalSocketTimeout = 30000

# the shutdownTimeout is the number of milliseconds the job should wait for
# the job to shut down correctly in the event of a manual stop. This timeout should be
# long enough for the current step to complete, so that other steps can be notified
# of the 'job stop' attempt and the job repository can be correctly updated.
# However, if the job hangs for unknown reasons, it's likely impossible to update the
# job repository successfully, meaning the current jobExecution will be in an unknown state.
shutdownTimeout = 20000

#number of partitions to generate
gridSize = 3

#ms for partition handler to complete partitioning before an exception is thrown
partitionHandlerTimeout = 10000000

#name of timestamp colum used for checking for new data
timeStampColumnNameToPersistInJobRepository = updateTime

#ms to scan forward for new data
#processingPeriod = 345600000

#if configured, overrides the grid size and sets the max number of rows per partition.
#Warning - if used, can result in large numbers of partitions per job
#maxPartitionSize = 1000

#if configured, queries the source database to ensure that each partition has at least 1 row of data.
#Helpful to avoid masses of empty partitions when connecting to a database with non-sequestial primary keys. However, also
#requires a count query to be performed, which might not always be desirable
#checkForEmptyPartitions = false
## POSTGRES TARGET DB CONFIGURATIONS
target.JdbcPath      = jdbc:postgresql://postgres:5432/cogstack
target.Driver        = org.postgresql.Driver
target.username      = cogstack
target.password      = mysecretpassword
target.idleTimeout   = 30000
target.maxLifetime   = 60000

## SOURCE TARGET DB CONFIGURATIONS
source.JdbcPath      = jdbc:postgresql://postgres:5432/cogstack
source.Driver        = org.postgresql.Driver
source.username      = cogstack
source.password      = mysecretpassword
source.idleTimeout   = 30000
source.maxLifetime   = 60000

## Job Repo DB CONFIGURATIONS
jobRepository.JdbcPath      = jdbc:postgresql://postgres:5432/cogstack
jobRepository.Driver        = org.postgresql.Driver
jobRepository.username      = cogstack
jobRepository.password      = mysecretpassword
jobRepository.idleTimeout   = 30000
jobRepository.maxLifetime   = 60000


datePatternForScheduling = yyyy-MM-dd HH:mm:ss.S
dbmsToJavaSqlTimestampType = TIMESTAMP



target.Sql           = INSERT INTO tblOutputDocs (srcColumnFieldName, srcTableName, primaryKeyFieldName, primaryKeyFieldValue, updateTime, output) VALUES (:srcColumnFieldName, :srcTableName, :primaryKeyFieldName, CAST ( :primaryKeyFieldValue AS integer), :timeStamp, :outputData)
source.selectClause = SELECT *
source.sortKey = primaryKeyFieldValue
source.pageSize = 2
source.fromClause = FROM tblInputDocs

#paging item reader configuration
columnToProcess = primaryKeyFieldValue
#this is the table containing the primary keys and timestamps
tableToPartition = tblInputDocs
pkColumnNameToPartition = primaryKeyFieldValue

#for DBLineFixer - concatenate text from multiple database rows
# primary key of original document
lf.documentKeyName = primaryKeyFieldValue
# primary key from table with multi row documents
lf.lineKeyName = line_id
# text content from table with multi row documents
lf.lineContents = line_text
#source table of multi row documents
lf.srcTableName = tblDocLines
# fieldname for lineFixer output
dbLineFixerFieldName = lineFixer
#####Scheduler CONFIGURATION####
#if true, run a new job after the last one has finished
useScheduling = slave
## scheduling (Cron style syntax)
scheduler.rate = */5 * * * * *

#if useTimeStampBasedScheduling ia true, process this number of milliseconds from last successful job/first timestamp/start date,
#depending on configuration and status of the JobRepository
processingPeriod = 77760000000
#####TIKA CONFIGURATION####

#TikaItemProcesser - XHTML or plaintext
keepTags = false
# field with binary content for tika conversion
binaryFieldName = someText
# fieldname for tika output
tikaFieldName = sometext
spring.profiles.active=deid,biolark,tika,remotePartitioning,jdbc,elasticsearch,primaryKeyAndTimeStampPartition
