# EXAMPLE CONFIG FOR deidentification Job 
#####Elasticsearch CONFIGURATION####
#ES cluster name
elasticsearch.cluster.name = elasticsearch
#ES cluster IP/hostname
elasticsearch.cluster.host =  localhost
#ES cluster transport port
elasticsearch.cluster.port = 9200
#ES the shield plugin (commercial)
elasticsearch.shield.enabled = false
#Shield username/password
elasticsearch.shield.user = <user>:<password>
#ES keystore for ssl
elasticsearch.shield.ssl.keystore.path = /home/rich/elk/ssh-keystores/tempnode/tempnode.jks
#ES keystore password
elasticsearch.shield.ssl.keystore.password = <password>
#ES Truststore (see Shield docs)
elasticsearch.shield.ssl.truststore.path = /home/rich/elk/ssh-keystores/tempnode/tempnode.jks
#ES Truststore password
elasticsearch.shield.ssl.truststore.password = <password>
#use encryption on transport layer
elasticsearch.shield.transport.ssl = true

##General ES options
#load data into this index
elasticsearch.index.name = test_index2
#load data into this type
elasticsearch.type = test_type
#Allow this many ms for cluster response
elasticsearch.response.timeout = 1000000
#If the input SQL query returns columns with these labels, ignore them
elasticsearch.excludeFromIndexing = binaryContent,primaryKeyFieldName
#specify the JODA date pattern that is compatible with the elasticsearch dynamic file mapping for dates (see ES docs on dates)
datePatternForES = yyyy-MM-dd'T'HH:mm:ss.SSS
#####DEIDENTIFICATION CONFIGURATION####
#mask text in documents, such as patient identifiers, using a GATE app or the KCL ElasticGazetteer


#Use a gate app for de-identification, rather than the ElasticGazetteer
deid.useGateApp = false

#GATE app location of de-identification app
deid.deIdApp = /home/rich/gate_apps/ElasticGazetteer/application.xgapp

#comma separated field names to process. Maps to database column labels
deid.fieldsToDeId = someText

#if true, replace the text strings in the fields listed above. If false, create a new field called deidentified_<originalFieldName>
deid.replaceFields = false




#ElasticGazetteer only

#levenstein distance between 1 and 100. Higher = greater fuzziness
deid.levDistance = 30

#don't mask words shorter than this many characters
deid.minWordLength = 3

#SQL query to retrieve strings of text for masking. E.g. names, addresses etc. Must surround document primary key
# as mapped elsewhere
deid.stringTermsSQLFront = select identifier from vwIdentifiers where primaryKeyFieldValue =

# second part of above SQL string (if required
deid.stringTermsSQLBack =

#As above, but with date/time values
deid.timestampTermsSQLFront = select DATE_OF_BIRTH from tblIdentifiers where primaryKeyFieldValue =

# second part of above SQL string (if required
deid.timestampTermsSQLBack =

#####NIOLARK CONFIGURATION####
webservice.endPoint = http://localhost:8080/gate

#comma seperated SQL columns to send to bioLark
webservice.fieldsToSendToWebservice = someText

#name of new field created by biolark (created a new field with the format <inputfield>_biolark
webservice.fieldName = bioyodie

#ms to connect to a biolark WS
webservice.connectTimeout = 10000

#ms for biolark to respond
webservice.readTimeout = 60000

#if biolark fails, retry for this long in ms before giving up
webservice.retryTimeout = 5000

#wait this many ms between retry attempts
webservice.retryBackoff = 3000

#name for debugging and info
webservice.name = bioyodie
#####JMS CONFIGURATION####
##Only required if using remote partitioning

#IP of JMS server
jms.ip = tcp://172.17.0.3:61616?jms.prefetchPolicy.all=1
jms.username = admin
jms.password = your_password
##timeout to prevent a hung client
jms.closeTimeout = 100000

#name of request channel (should be job unique)
jms.requestQueueName = basicReqChannel
#name of reply channel (should be job unique)
jms.replyQueueName = basicReplyQueue
#####JOB AND STEP CONFIGURATION####
# commit interval in step - process this many rows before committing results. default 50
step.chunkSize = 50
#number of exceptions that can be thrown before job fails. default 5
step.skipLimit = 5

#Asynchonous TaskExecutor Thread pool size - for multithreading partitions
step.concurrencyLimit = 16

#job should complete before this many ms or it will be listed as a failure.
job.jobTimeout = 10000000

#globally unique job name. default is defaultJob
job.jobName = exampleJob

#since some JDBC drivers don't support socket timeouts, they can't detect network failures.
#This workaround allows us to specify a global socket timeout for the JVM without diving into the OS socket configuration.
#However, the downside is that all sockets are affected in the JVM, so may not always be appropriate to set this. Comment out to
#not use this global setting
job.globalSocketTimeout = 30000



#Partitioner Configuration. This is used to inform how the total row count per job should be broken into
#seperate partitions

#Two partitioner types are available, either using primary keys (PK) or timestamps and primary keys (PKTimeStamp)
#If using the scheduler, the PKTimeStamp type should be configured
partitioner.partitionType = PKTimeStamp

#number of partitions to generate (x / total job row count)
partitioner.gridSize = 3

#ms for partition handler to complete partitioning before an exception is thrown
partitioner.partitionHandlerTimeout = 10000000

#name of timestamp column used for partitioning and checking for new data (only if scheduling is used)
partitioner.timeStampColumnName = updateTime

#name of PK column used for partitioning and checking for new data
#only use with scheduling if PKs are guaranteed to be generated sequentially
partitioner.pkColumnNameToPartition = primaryKeyFieldValue

#this is the table containing the primary keys and optionally, timestamps
partitioner.tableToPartition = tblInputDocs

# required prefix on some types of DB (e.g. SQL Server)
#partitioner.preFieldsSQL = TOP 100 PERCENT
##For meaning of ideTimeout and maxLifetime see Hikari Connection Pool docs

## POSTGRES TARGET DB CONFIGURATIONS
target.JdbcPath      = jdbc:postgresql://localhost:5432/cogstack
target.Driver        = org.postgresql.Driver
target.username      = cogstack
target.password      = mysecretpassword
target.idleTimeout   = 30000
target.maxLifetime   = 60000

## SOURCE TARGET DB CONFIGURATIONS
source.JdbcPath      = jdbc:postgresql://localhost:5432/cogstack
source.Driver        = org.postgresql.Driver
source.username      = cogstack
source.password      = mysecretpassword
source.idleTimeout   = 30000
source.maxLifetime   = 60000

## Job Repo DB CONFIGURATIONS
jobRepository.JdbcPath      = jdbc:postgresql://localhost:5432/cogstack
jobRepository.Driver        = org.postgresql.Driver
jobRepository.username      = cogstack
jobRepository.password      = mysecretpassword
jobRepository.idleTimeout   = 30000
jobRepository.maxLifetime   = 60000


#Depreciated
#datePatternForScheduling = yyyy-MM-dd HH:mm:ss.S

#Since different DBMS products interpret the SQL standard for time differently, is is necessary to explicitly specify
#the date type that the database is using. E.G. postgres=TIMESTAMP, SQL SERVER=DATETIME
source.dbmsToJavaSqlTimestampType = TIMESTAMP


#The principle SQL block that specifies data to process. Composed of three parts.
source.selectClause = SELECT *
source.sortKey = primaryKeyFieldValue
source.fromClause = FROM tblInputDocs

#If writing the cogstack constructed JSON to another DBMS as well as elasticsearch, this query specifies how it should be handled
target.Sql = INSERT INTO tblOutputDocs (srcColumnFieldName, srcTableName, primaryKeyFieldName, primaryKeyFieldValue, updateTime, output) VALUES (:srcColumnFieldName, :srcTableName, :primaryKeyFieldName, CAST ( :primaryKeyFieldValue AS integer), :timeStamp, :outputData)


##paging item reader configuration
#number of rows per page
source.pageSize = 2
#The name of the column to process (depreciated)
#columnToProcess = primaryKeyFieldValue

# DB column label mapping for table metadata. Required.
srcTableName = srctablename
srcColumnFieldName = srcColumnFieldName
primaryKeyFieldName = primaryKeyFieldName
primaryKeyFieldValue = primaryKeyFieldValue
timeStamp = updateTime


#for DBLineFixer - concatenate text from multiple database rows
# primary key of original document
lf.documentKeyName = primaryKeyFieldValue
# primary key from table with multi row documents
lf.lineKeyName = line_id
# text content from table with multi row documents
lf.lineContents = line_text
#source table of multi row documents
lf.srcTableName = tblDocLines
# fieldname for lineFixer output
lf.fieldName = lineFixer
#####Scheduler CONFIGURATION####
#if true, run a new job after the last one has finished - new jobs will continute to be created indefinitely
scheduler.useScheduling = true
## if above is true, new jobs will be created according to this CRON style schedule
scheduler.rate = */5 * * * * *

#if useTimeStampBasedScheduling is true, process this number of milliseconds from last successful job/first timestamp/start date,
#depending on configuration and status of the JobRepository. For instance, last timestamp in job1 is t0. job2 will then look for
#new data where t0 < timestamp <= t0+processingPeriod. Defaults to 77760000000 ms (9000 days)
scheduler.processingPeriod = 777600000000

spring.profiles.active=deid,bioyodie,localPartitioning,jdbc_in,elasticsearch,primaryKeyAndTimeStampPartition